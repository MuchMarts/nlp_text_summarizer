{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuchMarts/nlp_text_summarizer/blob/main/mini_project_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q-beJmxP6s-"
      },
      "source": [
        "# Mini Project: Lecture Note Summarizer\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Authors**:  Mārtiņš Patjanko (*mp22042*); Dinh Phuoc Nguyen Tran (*dt22025*)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Lecture note summarizer that is implemeted in 2 steps - Extractive and Abstracive.\n",
        "\n",
        "Extractive step imlpements TextRank to get the most important sentences.\n",
        "\n",
        "Abstracive step implements T5 transformer to summarize in natural language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKccZlzcRXO4"
      },
      "source": [
        "# Data Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjGb61Jbb5Xb"
      },
      "source": [
        "All data is stored on github. This installs the newest version of the repository, uzips it and removes unnesessary files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wygK396QRado",
        "outputId": "ef73745d-3018-4f98-c222-3cf5ccbc37b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  temp.zip\n",
            "8efd362f04d758df50a41c94fcee93c33ec9201e\n",
            "  inflating: input_folder/README.md  \n",
            "  inflating: input_folder/config.yaml  \n",
            "  inflating: input_folder/genderdisparities_notes.txt  \n",
            "  inflating: input_folder/genderdisparities_notes_summarized.txt  \n",
            "  inflating: input_folder/lecture8.txt  \n",
            "  inflating: input_folder/lecture8_summarized.txt  \n",
            "  inflating: input_folder/lecture9.txt  \n",
            "  inflating: input_folder/lecture9_summarized.txt  \n",
            "  inflating: input_folder/lecturenotes_sample.txt  \n",
            "  inflating: input_folder/lecturenotes_sample_summarized.txt  \n",
            "  inflating: input_folder/notes_lec_1.txt  \n",
            "  inflating: input_folder/notes_lec_10.txt  \n",
            "  inflating: input_folder/notes_lec_11.txt  \n",
            "  inflating: input_folder/notes_lec_12.txt  \n",
            "  inflating: input_folder/notes_lec_2.txt  \n",
            "  inflating: input_folder/notes_lec_3.txt  \n",
            "  inflating: input_folder/notes_lec_4.txt  \n",
            "  inflating: input_folder/notes_lec_5.txt  \n",
            "  inflating: input_folder/notes_lec_6.txt  \n",
            "  inflating: input_folder/notes_lec_7.txt  \n",
            "  inflating: input_folder/notes_lec_8.txt  \n",
            "  inflating: input_folder/notes_lec_9.txt  \n",
            "  inflating: input_folder/presentationnotes.txt  \n",
            "  inflating: input_folder/presentationnotes_summarized.txt  \n",
            "  inflating: input_folder/sample.txt  \n",
            "  inflating: input_folder/sample_summarized.txt  \n",
            "  inflating: input_folder/textreview_notes.docx  \n",
            "  inflating: input_folder/textreview_notes_summarized.docx  \n",
            "  inflating: input_folder/transcript_lec_1.txt  \n",
            "  inflating: input_folder/transcript_lec_10.txt  \n",
            "  inflating: input_folder/transcript_lec_11.txt  \n",
            "  inflating: input_folder/transcript_lec_12.txt  \n",
            "  inflating: input_folder/transcript_lec_2.txt  \n",
            "  inflating: input_folder/transcript_lec_3.txt  \n",
            "  inflating: input_folder/transcript_lec_4.txt  \n",
            "  inflating: input_folder/transcript_lec_5.txt  \n",
            "  inflating: input_folder/transcript_lec_6.txt  \n",
            "  inflating: input_folder/transcript_lec_7.txt  \n",
            "  inflating: input_folder/transcript_lec_8.txt  \n",
            "  inflating: input_folder/transcript_lec_9.txt  \n",
            "  inflating: input_folder/transcript_sample.txt  \n",
            "  inflating: input_folder/transcript_sample_summarized.txt  \n",
            "  inflating: input_folder/vietnameselang.txt  \n",
            "  inflating: input_folder/vietnameselang_summarized.txt  \n",
            "  inflating: input_folder/virtuallearningnotes.docx  \n",
            "  inflating: input_folder/virtuallearningnotes_summarized.docx  \n",
            "  inflating: input_folder/helper.py  \n",
            " extracting: input_folder/lecture_summary_map.yaml  \n",
            " extracting: input_folder/requirements.txt  \n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.1)\n"
          ]
        }
      ],
      "source": [
        "!rm -rf sample_data\n",
        "!rm -rf your_folder\n",
        "!rm -rf input_folder\n",
        "\n",
        "!wget -qO temp.zip https://github.com/MuchMarts/nlp_text_summarizer/archive/refs/heads/main.zip && \\\n",
        "unzip -j temp.zip -d input_folder && rm temp.zip\n",
        "\n",
        "!pip install python-docx\n",
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US7PD8LrcriA"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "from enum import Enum\n",
        "# Read other document formats\n",
        "import fitz\n",
        "from docx import Document\n",
        "\n",
        "\n",
        "class Output_Types(Enum):\n",
        "  TEXTRANK = 1\n",
        "  T5 = 2\n",
        "  TEXTRANK_T5 = 3\n",
        "\n",
        "Type_Mapping = {\n",
        "    Output_Types.TEXTRANK : \"textrank\",\n",
        "    Output_Types.T5 : \"t5\",\n",
        "    Output_Types.TEXTRANK_T5 : \"textrank_t5\"\n",
        "}\n",
        "\n",
        "# Output_Types.TEXTRANK\n",
        "# Output_Types.T5\n",
        "# Output_Types.TEXTRANK_T5\n",
        "\n",
        "INPUT_DIRECTORY = 'input_folder'\n",
        "OUTPUT_DIRECTORY = 'output_folder'\n",
        "\n",
        "with open(INPUT_DIRECTORY + \"/config.yaml\") as f:\n",
        "  CONFIG = yaml.safe_load(f)\n",
        "\n",
        "if CONFIG is None:\n",
        "  raise Exception(\"Config file not found\")\n",
        "\n",
        "def store_file(data, output_type: Output_Types, name, output_dir=OUTPUT_DIRECTORY):\n",
        "  if not os.path.exists(output_dir + \"/\" + Type_Mapping[output_type]):\n",
        "    os.makedirs(output_dir + \"/\" + Type_Mapping[output_type])\n",
        "\n",
        "  with open(output_dir + \"/\" + Type_Mapping[output_type] + \"/\" + name, 'w') as f:\n",
        "    f.write(data)\n",
        "    f.close()\n",
        "\n",
        "def load_file(name, input_dir=INPUT_DIRECTORY):\n",
        "  path = input_dir + \"/\" + name\n",
        "  ext = os.path.splitext(path)[1].lower()\n",
        "\n",
        "  if ext == '.txt':\n",
        "    with open(path, 'r') as f:\n",
        "      data = f.read()\n",
        "      f.close()\n",
        "      return data\n",
        "\n",
        "  if ext == '.docx':\n",
        "    doc = Document(path)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "  if ext == '.pdf':\n",
        "    doc = fitz.open(path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "      text += page.get_text()\n",
        "    return text\n",
        "\n",
        "  raise Exception(f\"Unsupported file type: {ext}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP8TrIRNQZwU"
      },
      "source": [
        "## Extractive Step"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency `spacy` is an NLP pipeline for parsing and analyzing text.\n",
        "Dependency `pytextrank` implements the TextRank algorithm"
      ],
      "metadata": {
        "id": "dFN6amSUgkd_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LTNBsRLMXKo",
        "outputId": "2ee6abb7-6ad2-45c1-a1cf-6a893b5adbc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install --quiet numpy nltk pytextrank spacy torch\n",
        "!pip install scipy>=1.14.0 --quiet\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXR1vQ31gacF",
        "outputId": "7cd0b303-49bb-41fc-cccc-cbde0600c6f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pytextrank.base.BaseTextRankFactory at 0x78aebd760590>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "import spacy\n",
        "import pytextrank\n",
        "\n",
        "# NLP\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe(\"textrank\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amcVAEu58mXR"
      },
      "outputs": [],
      "source": [
        "def pytextrank_summarize(text, top_n=3):\n",
        "    doc = nlp(text)\n",
        "    summary = [sent.text for sent in doc._.textrank.summary(limit_phrases=15, limit_sentences=top_n)]\n",
        "    return ' '.join(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd3C2nLGP32l"
      },
      "source": [
        "# Abstractive Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZtlz5rPSE38"
      },
      "source": [
        "Dependency `transformers` is used to get the T5 transformer and its tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seO24dBRR03_"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install --quiet transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7AgkIU7TCYn"
      },
      "outputs": [],
      "source": [
        "# Config values, transformer\n",
        "MODEL_NAME = \"t5-small\"\n",
        "#MODEL_NAME = \"Vamsi/T5_Paraphrase_Paws\"\n",
        "#MODEL_NAME = \"google/flan-t5-small\"\n",
        "MAX_TOKENS = 512 # T5 is trained on 512 token inputs, its the max input\n",
        "MIN_TOKENS = 100\n",
        "INSTRUCTION = \"paraphrase: \"\n",
        "#INSTRUCTION = \"Paraphrase the following text to improve clarity and grammar, but keep all details and information unchanged: \"\n",
        "\n",
        "DEBUG = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maR-otmoSYx1"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, logging\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTy5DCk4sNQW"
      },
      "source": [
        "Input text might be longer than 512 tokens, for this we will implement sliding-window chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjQyuGAvs88E"
      },
      "outputs": [],
      "source": [
        "# Config values, chunking\n",
        "CHUNKER = \"sliding_window\"\n",
        "#CHUNKER = \"sentence_sliding_window\"\n",
        "CHUNK_SIZE = 450\n",
        "OVERLAP = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgkS-5HAxV_J"
      },
      "source": [
        "Helper function to time each function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IegxMulIxDBR"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "def timer(fn):\n",
        "    @wraps(fn)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = fn(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f\"DEBUG: {fn.__name__!r} took {end - start:.4f} sec\")\n",
        "        return result\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3i_aOQXsNAa"
      },
      "outputs": [],
      "source": [
        "def generate_sliding_window_chunks(text):\n",
        "  tokens = tokenizer.encode(text)\n",
        "  chunks = []\n",
        "  start = 0\n",
        "  while start < len(tokens):\n",
        "    end = min(start + CHUNK_SIZE, len(tokens))\n",
        "    chunks.append(tokens[start:end])\n",
        "    start += CHUNK_SIZE - OVERLAP\n",
        "  if DEBUG: print(f\"DEBUG: Text Length: {len(text)}, Token Count: {len(tokens)}, Chunk Count: {len(chunks)}\")\n",
        "\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNdObDjovGyO"
      },
      "outputs": [],
      "source": [
        "def generate_summary(text, chunker=CHUNKER, instruction=INSTRUCTION, max_tokens=MAX_TOKENS, min_length=MIN_TOKENS, do_sample=True, temperature=0.9):\n",
        "  text = text.replace('\\n', ' ')\n",
        "  chunks = generate_sliding_window_chunks(text)\n",
        "  summary_chunks = []\n",
        "\n",
        "  for chunk in chunks:\n",
        "    input_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
        "    input_ids = tokenizer(\n",
        "        instruction + input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_tokens,\n",
        "        truncation=True)\n",
        "\n",
        "    summary_ids = model.generate(\n",
        "        input_ids.input_ids,\n",
        "        num_beams=4,\n",
        "        max_length=max_tokens,\n",
        "        min_length=min_length,\n",
        "        early_stopping=True,\n",
        "        do_sample=do_sample,\n",
        "        top_p=0.95,\n",
        "        temperature=temperature\n",
        "        )\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    summary_chunks.append(summary)\n",
        "  summaries = ' \\n'.join(summary_chunks)\n",
        "  if DEBUG: print(f\"DEBUG: Text Length: {len(text)}, Summary Length: {len(summaries)}\")\n",
        "  return summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vukta-mNTT8L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RmBy1-HzXQ0"
      },
      "source": [
        "# Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjeAuj9Djjnr"
      },
      "source": [
        "## Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwc8AXxQ2b-3",
        "outputId": "113dd2ae-9d5e-4f93-f6a4-566f2bfee8c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "!pip install --quiet rouge-score\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1XzkziA2PfG"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRRrxYYO3kMp"
      },
      "outputs": [],
      "source": [
        "def rouge_evaluation(prediction, reference, eval_print = True):\n",
        "  scores = scorer.score(target=reference, prediction=prediction)\n",
        "  if eval_print:\n",
        "    print(f\"############# EVALUATION #############\")\n",
        "    print(f\"ROUGE-1: {scores['rouge1']}\")\n",
        "    print(f\"ROUGE-2: {scores['rouge2']}\")\n",
        "    print(f\"ROUGE-L: {scores['rougeL']}\")\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9p7bHBQ4pAR"
      },
      "outputs": [],
      "source": [
        "#predictions = \"The cat sat on the mat.\"\n",
        "#references = \"The cat is sitting on the mat.\"\n",
        "#scores = rouge_evaluation(predictions, references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNGl-kMY5Wvf"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtrbsvBe5ghG"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "\n",
        "def bert_score_evaluation(predictions, references, eval_print = True):\n",
        "  P, R, F1 = score(predictions, references, lang=\"en\", verbose=False)\n",
        "  if eval_print:\n",
        "    print(f\"############# EVALUATION #############\")\n",
        "    print(\"BERTScore F1:\", F1.mean().item())\n",
        "    print(\"BERTScore P:\", P.mean().item())\n",
        "    print(\"BERTScore R:\", R.mean().item())\n",
        "  return {\"F1\" : F1, \"R\" : R, \"P\" : P}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrSpeANm591c"
      },
      "outputs": [],
      "source": [
        "#bert_score_evaluation([predictions], [references])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5rKjuk4jlr4"
      },
      "source": [
        "## Evaluation for each Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXBfLWAIznSR"
      },
      "source": [
        "### HELPERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGWIEnHCzl6B"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from nltk import sent_tokenize\n",
        "# Formats a string for stats, per entry\n",
        "def eval_score_format_helper(key, scores):\n",
        "  score = \"[ \" + key + \" ] - \"\n",
        "\n",
        "  rouge = scores[key][\"rouge\"]\n",
        "  bertscore = scores[key][\"bertscore\"]\n",
        "\n",
        "  score += f\"ROUGE 1: {rouge['rouge1'].fmeasure} \"\n",
        "  score += f\"2: {rouge['rouge2'].fmeasure} \"\n",
        "  score += f\"L: {rouge['rougeL'].fmeasure} \"\n",
        "\n",
        "  score += f\"BERTScore F1: {bertscore['F1'].mean().item()} \"\n",
        "  score += f\"P: {bertscore['P'].mean().item()} \"\n",
        "  score += f\"R: {bertscore['R'].mean().item()} \"\n",
        "\n",
        "  return score\n",
        "\n",
        "# Prints out all evaluation scores for a entries and calculates the total average\n",
        "def print_eval_scores(scores):\n",
        "  average = {\n",
        "      'rouge' : {\n",
        "          'r1' : 0,\n",
        "          'r2' : 0,\n",
        "          'rl' : 0\n",
        "      },\n",
        "      'bertscore' : {\n",
        "          'f1' : 0,\n",
        "          'p' : 0,\n",
        "          'r' : 0\n",
        "      }\n",
        "  }\n",
        "  print(\"Score for each dataset entry\")\n",
        "  for pair in CONFIG[\"files\"]:\n",
        "    print(eval_score_format_helper(pair[\"original\"], scores=scores))\n",
        "\n",
        "    average['rouge']['r1'] += scores[pair[\"original\"]][\"rouge\"][\"rouge1\"].fmeasure\n",
        "    average['rouge']['r2'] += scores[pair[\"original\"]][\"rouge\"][\"rouge2\"].fmeasure\n",
        "    average['rouge']['rl'] += scores[pair[\"original\"]][\"rouge\"][\"rougeL\"].fmeasure\n",
        "\n",
        "    average['bertscore']['f1'] += scores[pair[\"original\"]][\"bertscore\"][\"F1\"].mean().item()\n",
        "    average['bertscore']['p'] += scores[pair[\"original\"]][\"bertscore\"][\"P\"].mean().item()\n",
        "    average['bertscore']['r'] += scores[pair[\"original\"]][\"bertscore\"][\"R\"].mean().item()\n",
        "\n",
        "  print(f\"Average Result: \")\n",
        "  print(f\"ROUGE 1: {average['rouge']['r1'] / len(CONFIG['files'])}\")\n",
        "  print(f\"ROUGE 2: {average['rouge']['r2'] / len(CONFIG['files'])}\")\n",
        "  print(f\"ROUGE L: {average['rouge']['rl'] / len(CONFIG['files'])}\")\n",
        "  print(f\"BERTScore F1: {average['bertscore']['f1'] / len(CONFIG['files'])}\")\n",
        "  print(f\"BERTScore P: {average['bertscore']['p'] / len(CONFIG['files'])}\")\n",
        "  print(f\"BERTScore R: {average['bertscore']['r'] / len(CONFIG['files'])}\")\n",
        "\n",
        "def prec_to_sent(text, precentage):\n",
        "  return math.ceil(len(sent_tokenize(text)) * precentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W733iO2oowYH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfZJLVF3kC1e"
      },
      "source": [
        "### EVALUATION PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUxe4PSlkBrF"
      },
      "outputs": [],
      "source": [
        "#TEXTRANK_PRECENTAGES = [ 0.2, 0.5, 0.8 ]\n",
        "TEXTRANK_PRECENTAGES = [ 0.3 ]\n",
        "T5_INSTRUCTION = [ INSTRUCTION ]\n",
        "#T5_MIN_LENGTH = [ None, 100 ]\n",
        "T5_MIN_LENGTH = [ 100 ]\n",
        "T5_CHUNKER = [ \"sliding_window\" ]\n",
        "#T5_DO_SAMPLE = [ False, True]\n",
        "T5_DO_SAMPLE = [ True ]\n",
        "#T5_TEMPERATURE = [ None, 0.9 ]\n",
        "T5_TEMPERATURE = [ 0.9 ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uILrxDJPjpC0"
      },
      "source": [
        "### Textrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdKPnbCTjoG1"
      },
      "outputs": [],
      "source": [
        "# Stores calculated scores\n",
        "textrank_scores = {}\n",
        "\n",
        "@timer\n",
        "def evaluate_textrank():\n",
        "    for pair in CONFIG[\"files\"]:\n",
        "\n",
        "        data = load_file(pair[\"original\"])\n",
        "        comparison = load_file(pair[\"summary\"])\n",
        "\n",
        "        if data is None or comparison is None:\n",
        "            raise Exception(f\"File not found: {pair['original']} or {pair['summary']}\")\n",
        "\n",
        "        # Generates summary with TextRank\n",
        "        textrank_summary = pytextrank_summarize(data, top_n=3)\n",
        "\n",
        "        # Evaluates summary with rouge and bert\n",
        "        textrank_scores[pair[\"original\"]] = {\n",
        "          \"rouge\": rouge_evaluation(textrank_summary, comparison, eval_print=False),\n",
        "          \"bertscore\": bert_score_evaluation([textrank_summary], [comparison], eval_print=False)\n",
        "        }\n",
        "\n",
        "        # Store generated summary file\n",
        "        store_file(textrank_summary, Output_Types.TEXTRANK, \"textrank_summary_\" + pair[\"original\"])\n",
        "\n",
        "#evaluate_textrank() # Runs evaluation\n",
        "#print_eval_scores(textrank_scores) # Outputs to console scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8hVKOkbjrKu"
      },
      "source": [
        "### T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjWg63KWjv_M"
      },
      "outputs": [],
      "source": [
        "t5_scores = {}\n",
        "\n",
        "@timer\n",
        "def evaluate_t5():\n",
        "  for pair in CONFIG[\"files\"]:\n",
        "    data = load_file(pair[\"original\"])\n",
        "    comparison = load_file(pair[\"summary\"])\n",
        "\n",
        "    if data is None or comparison is None:\n",
        "      raise Exception(f\"File not found: {data} or {comparison}\")\n",
        "\n",
        "    summary = generate_summary(data)\n",
        "\n",
        "    t5_scores[pair[\"original\"]] = {\n",
        "      \"rouge\": rouge_evaluation(summary, comparison, eval_print=False),\n",
        "      \"bertscore\": bert_score_evaluation([summary], [comparison], eval_print=False)\n",
        "    }\n",
        "\n",
        "    store_file(summary, Output_Types.T5, \"t5_summary_\" + pair[\"original\"])\n",
        "\n",
        "#evaluate_t5()\n",
        "#print_eval_scores(t5_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B_bDCz8jsek"
      },
      "source": [
        "### Textrank + T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9b1v-08jwi-"
      },
      "outputs": [],
      "source": [
        "textrank_t5 = {}\n",
        "\n",
        "@timer\n",
        "def evaluate_textrank_t5():\n",
        "    for pair in CONFIG[\"files\"]:\n",
        "        data = load_file(pair[\"original\"])\n",
        "        comparison = load_file(pair[\"summary\"])\n",
        "\n",
        "        if data is None or comparison is None:\n",
        "            raise Exception(f\"File not found: {pair['original']} or {pair['summary']}\")\n",
        "\n",
        "        # Generates a summary with TextRank, then using that Generates a summary with T5\n",
        "        # This combines both aproaches, for hopefully a better outcome\n",
        "        textrank_summary = pytextrank_summarize(data, top_n=3)  # or manual_textrank()\n",
        "        t5_summary = generate_summary(textrank_summary)  # Your T5 summary function\n",
        "\n",
        "        textrank_t5[pair[\"original\"]] = {\n",
        "            \"rouge\": rouge_evaluation(t5_summary, comparison, eval_print=False),\n",
        "            \"bertscore\": bert_score_evaluation([t5_summary], [comparison], eval_print=False)\n",
        "        }\n",
        "\n",
        "#evaluate_textrank_t5()\n",
        "#print_eval_scores(textrank_t5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHk3EfkDcdmM"
      },
      "source": [
        "### All evals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoSa3aXOllN_"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "evaluation_results = defaultdict(list)\n",
        "\n",
        "def evaluation_pipeline(mode,\n",
        "                        textrank_prec=None,\n",
        "                        t5_instruct=None, t5_min_len=None,\n",
        "                        t5_chunker=None, t5_do_sample=None, t5_temp=None):\n",
        "\n",
        "  counter = 0 # Used to artifically stop eval pipeline faster\n",
        "  for pair in CONFIG[\"files\"]:\n",
        "    if counter == 4: break\n",
        "    if \"type\" not in pair: continue # Only use pairs where a type is defined, just used to have finer control over eval data\n",
        "\n",
        "    print(\".\", end=\"\") #DEBUG SHOW STUFF HAPPENING\n",
        "    data = load_file(pair[\"original\"])\n",
        "    comparison = load_file(pair[\"summary\"])\n",
        "\n",
        "    if data is None or comparison is None:\n",
        "      print(f\"File not found: {data} or {comparison}; Skipping...\")\n",
        "      continue\n",
        "\n",
        "    if mode == \"textrank\":\n",
        "      summary = pytextrank_summarize(data, top_n=prec_to_sent(data, textrank_prec))\n",
        "    elif mode == \"t5\":\n",
        "      summary = generate_summary(data,\n",
        "                                 instruction=t5_instruct,\n",
        "                                 min_length=t5_min_len,\n",
        "                                 chunker=t5_chunker,\n",
        "                                 do_sample=t5_do_sample,\n",
        "                                 temperature=t5_temp)\n",
        "    elif mode == \"combined\":\n",
        "      summary = pytextrank_summarize(data, top_n=prec_to_sent(data, textrank_prec))\n",
        "      summary = generate_summary(summary,\n",
        "                                 instruction=t5_instruct,\n",
        "                                 min_length=t5_min_len,\n",
        "                                 chunker=t5_chunker,\n",
        "                                 do_sample=t5_do_sample,\n",
        "                                 temperature=t5_temp)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid mode\")\n",
        "\n",
        "    rouge = rouge_evaluation(summary, comparison, eval_print=False)\n",
        "    bert = bert_score_evaluation([summary], [comparison], eval_print=False)\n",
        "\n",
        "    evaluation_results[mode].append({\n",
        "        \"file\": pair[\"original\"],\n",
        "        \"rouge1_f1\": rouge[\"rouge1\"].fmeasure,\n",
        "        \"rouge2_f1\": rouge[\"rouge2\"].fmeasure,\n",
        "        \"rougeL_f1\": rouge[\"rougeL\"].fmeasure,\n",
        "        \"bertscore_f1\": bert[\"F1\"].mean().item(),\n",
        "        \"params\": {\n",
        "            \"textrank_percent\": textrank_prec,\n",
        "            \"t5_instruction\": t5_instruct,\n",
        "            \"t5_min_length\": t5_min_len,\n",
        "            \"t5_chunker\": t5_chunker,\n",
        "            \"t5_do_sample\": t5_do_sample,\n",
        "            \"t5_temp\": t5_temp\n",
        "        }\n",
        "    })\n",
        "    #counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnH1QrhclAyp",
        "outputId": "86afaf2c-2dde-4da5-834b-5744899408f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............------------------------------\n",
            "DEBUG: 'textrank_eval_pipeline' took 156.1664 sec\n",
            "------------------------------\n",
            "............------------------------------\n",
            "DEBUG: 't5_eval_pipeline' took 2635.4274 sec\n",
            "------------------------------\n",
            "............------------------------------\n",
            "DEBUG: 'combined_eval_pipeline' took 975.5115 sec\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "\n",
        "# Textrank\n",
        "@timer\n",
        "def textrank_eval_pipeline():\n",
        "  for textrank_percentage in TEXTRANK_PRECENTAGES:\n",
        "    evaluation_pipeline(\n",
        "      mode=\"textrank\",\n",
        "      textrank_prec=textrank_percentage\n",
        "    )\n",
        "  print(\"------------------------------\")\n",
        "\n",
        "# T5\n",
        "@timer\n",
        "def t5_eval_pipeline():\n",
        "  print(\"------------------------------\")\n",
        "  for t5_instruction, t5_min_length, t5_chunker, t5_do_sample, t5_temperature in product(\n",
        "      T5_INSTRUCTION,\n",
        "      T5_MIN_LENGTH,\n",
        "      T5_CHUNKER,\n",
        "      T5_DO_SAMPLE,\n",
        "      T5_TEMPERATURE\n",
        "  ):\n",
        "    evaluation_pipeline(\n",
        "      mode=\"t5\",\n",
        "      t5_instruct=t5_instruction,\n",
        "      t5_min_len=t5_min_length,\n",
        "      t5_chunker=t5_chunker,\n",
        "      t5_do_sample=t5_do_sample,\n",
        "      t5_temp=t5_temperature\n",
        "    )\n",
        "  print(\"------------------------------\")\n",
        "\n",
        "# Combined\n",
        "@timer\n",
        "def combined_eval_pipeline():\n",
        "  print(\"------------------------------\")\n",
        "  for textrank_percentage, t5_instruction, t5_min_length, t5_chunker, t5_do_sample, t5_temperature in product(\n",
        "      TEXTRANK_PRECENTAGES,\n",
        "      T5_INSTRUCTION,\n",
        "      T5_MIN_LENGTH,\n",
        "      T5_CHUNKER,\n",
        "      T5_DO_SAMPLE,\n",
        "      T5_TEMPERATURE\n",
        "  ):\n",
        "    evaluation_pipeline(\n",
        "      mode=\"combined\",\n",
        "      textrank_prec=textrank_percentage,\n",
        "      t5_instruct=t5_instruction,\n",
        "      t5_min_len=t5_min_length,\n",
        "      t5_chunker=t5_chunker,\n",
        "      t5_do_sample=t5_do_sample,\n",
        "      t5_temp=t5_temperature\n",
        "    )\n",
        "  print(\"------------------------------\")\n",
        "\n",
        "evaluation_results = defaultdict(list)\n",
        "textrank_eval_pipeline()\n",
        "t5_eval_pipeline()\n",
        "combined_eval_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIoKAMfSrbqQ"
      },
      "source": [
        "### Analyze results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulFQhlSarbSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db766720-1ec0-41c9-9dad-0cd0c2c2dd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        mode                   file  rouge1_f1  rouge2_f1  rougeL_f1  \\\n",
            "19        t5   transcript_lec_8.txt   0.492634   0.142612   0.180884   \n",
            "7   textrank   transcript_lec_8.txt   0.437175   0.141328   0.143076   \n",
            "12        t5   transcript_lec_1.txt   0.474430   0.104809   0.174985   \n",
            "17        t5   transcript_lec_6.txt   0.462238   0.111155   0.176354   \n",
            "18        t5   transcript_lec_7.txt   0.463100   0.107055   0.158573   \n",
            "14        t5   transcript_lec_3.txt   0.414145   0.114557   0.188115   \n",
            "16        t5   transcript_lec_5.txt   0.419536   0.111111   0.182713   \n",
            "15        t5   transcript_lec_4.txt   0.390678   0.092053   0.163140   \n",
            "20        t5   transcript_lec_9.txt   0.434349   0.116716   0.172333   \n",
            "6   textrank   transcript_lec_7.txt   0.438185   0.101775   0.111111   \n",
            "0   textrank   transcript_lec_1.txt   0.437410   0.115174   0.120863   \n",
            "22        t5  transcript_lec_11.txt   0.429228   0.103175   0.164509   \n",
            "5   textrank   transcript_lec_6.txt   0.445214   0.101040   0.126527   \n",
            "10  textrank  transcript_lec_11.txt   0.393049   0.122434   0.123868   \n",
            "8   textrank   transcript_lec_9.txt   0.378537   0.138116   0.130732   \n",
            "31  combined   transcript_lec_8.txt   0.388646   0.091803   0.132096   \n",
            "21        t5  transcript_lec_10.txt   0.411690   0.096761   0.158957   \n",
            "26  combined   transcript_lec_3.txt   0.411285   0.086629   0.116614   \n",
            "3   textrank   transcript_lec_4.txt   0.345407   0.091387   0.103937   \n",
            "23        t5  transcript_lec_12.txt   0.378853   0.076525   0.143082   \n",
            "\n",
            "    bertscore_f1  combined_score  param_textrank_percent param_t5_instruction  \\\n",
            "19      0.797435        0.587278                     NaN         paraphrase:    \n",
            "7       0.798030        0.575028                     0.3                 None   \n",
            "12      0.790762        0.575020                     NaN         paraphrase:    \n",
            "17      0.791080        0.574614                     NaN         paraphrase:    \n",
            "18      0.792768        0.572824                     NaN         paraphrase:    \n",
            "14      0.789885        0.569506                     NaN         paraphrase:    \n",
            "16      0.789291        0.568689                     NaN         paraphrase:    \n",
            "15      0.803377        0.568142                     NaN         paraphrase:    \n",
            "20      0.783243        0.566399                     NaN         paraphrase:    \n",
            "6       0.795903        0.564351                     0.3                 None   \n",
            "0       0.788958        0.563168                     0.3                 None   \n",
            "22      0.777951        0.559692                     NaN         paraphrase:    \n",
            "5       0.777130        0.555982                     0.3                 None   \n",
            "10      0.784273        0.555811                     0.3                 None   \n",
            "8       0.782341        0.555722                     0.3                 None   \n",
            "31      0.788228        0.554609                     0.3         paraphrase:    \n",
            "21      0.775852        0.554499                     NaN         paraphrase:    \n",
            "26      0.785525        0.553252                     0.3         paraphrase:    \n",
            "3       0.794143        0.548583                     0.3                 None   \n",
            "23      0.780423        0.548049                     NaN         paraphrase:    \n",
            "\n",
            "    param_t5_min_length param_t5_chunker param_t5_do_sample  param_t5_temp  \n",
            "19                100.0   sliding_window               True            0.9  \n",
            "7                   NaN             None               None            NaN  \n",
            "12                100.0   sliding_window               True            0.9  \n",
            "17                100.0   sliding_window               True            0.9  \n",
            "18                100.0   sliding_window               True            0.9  \n",
            "14                100.0   sliding_window               True            0.9  \n",
            "16                100.0   sliding_window               True            0.9  \n",
            "15                100.0   sliding_window               True            0.9  \n",
            "20                100.0   sliding_window               True            0.9  \n",
            "6                   NaN             None               None            NaN  \n",
            "0                   NaN             None               None            NaN  \n",
            "22                100.0   sliding_window               True            0.9  \n",
            "5                   NaN             None               None            NaN  \n",
            "10                  NaN             None               None            NaN  \n",
            "8                   NaN             None               None            NaN  \n",
            "31                100.0   sliding_window               True            0.9  \n",
            "21                100.0   sliding_window               True            0.9  \n",
            "26                100.0   sliding_window               True            0.9  \n",
            "3                   NaN             None               None            NaN  \n",
            "23                100.0   sliding_window               True            0.9  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def build_eval_df(results_dict):\n",
        "    rows = []\n",
        "    for mode, entries in results_dict.items():\n",
        "        for entry in entries:\n",
        "            row = {\n",
        "                \"mode\": mode,\n",
        "                \"file\": entry[\"file\"],\n",
        "                \"rouge1_f1\": entry[\"rouge1_f1\"],\n",
        "                \"rouge2_f1\": entry[\"rouge2_f1\"],\n",
        "                \"rougeL_f1\": entry[\"rougeL_f1\"],\n",
        "                \"bertscore_f1\": entry[\"bertscore_f1\"],\n",
        "                # Flatten params dict here:\n",
        "                **{f\"param_{k}\": v for k, v in entry.get(\"params\", {}).items()}\n",
        "            }\n",
        "            rows.append(row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "eval_df = build_eval_df(evaluation_results)\n",
        "#print(eval_df)\n",
        "eval_df[\"rouge_avg_f1\"] = eval_df[[\"rouge1_f1\", \"rouge2_f1\", \"rougeL_f1\"]].mean(axis=1)\n",
        "eval_df[\"combined_score\"] = (eval_df[\"rouge_avg_f1\"] * 0.4 + eval_df[\"bertscore_f1\"] * 0.6)\n",
        "best = eval_df.sort_values(\"combined_score\", ascending=False).head(20)\n",
        "print(best[[\n",
        "    \"mode\",\n",
        "    \"file\",\n",
        "    \"rouge1_f1\",\n",
        "    \"rouge2_f1\",\n",
        "    \"rougeL_f1\",\n",
        "    \"bertscore_f1\",\n",
        "    \"combined_score\"\n",
        "] + [col for col in best.columns if col.startswith(\"param_t5_\") or \"param_textrank\" in col]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average by mode"
      ],
      "metadata": {
        "id": "wVwO2jRB6siL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mode_avg_scores = eval_df.groupby(\"mode\")[[\n",
        "    \"rouge1_f1\", \"rouge2_f1\", \"rougeL_f1\", \"bertscore_f1\", \"combined_score\"\n",
        "]].mean().reset_index()\n",
        "\n",
        "mode_avg_scores = mode_avg_scores.round(4)\n",
        "\n",
        "mode_avg_scores = mode_avg_scores.sort_values(\"combined_score\", ascending=False)\n",
        "\n",
        "print(mode_avg_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5wUkNIn6pqz",
        "outputId": "5bf971ae-3ca4-49c0-c1ba-475571d352e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       mode  rouge1_f1  rouge2_f1  rougeL_f1  bertscore_f1  combined_score\n",
            "1        t5     0.4288     0.1047     0.1685        0.7867          0.5656\n",
            "2  textrank     0.3958     0.1059     0.1181        0.7830          0.5525\n",
            "0  combined     0.3447     0.0689     0.1070        0.7812          0.5381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltO_eQzPrdPi"
      },
      "source": [
        "Simple eval, to console"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZUQLEVX8f4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e654f405-7a34-4610-fb89-7ec78c3af520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: 'generate_summary' took 35.2240 sec\n",
            "DEBUG: 'generate_summary' took 42.3224 sec\n",
            "DEBUG: 'generate_summary' took 7.5866 sec\n",
            "DEBUG: 'generate_summary' took 24.2273 sec\n",
            "DEBUG: 'generate_summary' took 8.1072 sec\n",
            "DEBUG: 'generate_summary' took 23.6568 sec\n",
            "DEBUG: 'evaluate_t5' took 178.1137 sec\n",
            "DEBUG: 'evaluate_textrank' took 37.1645 sec\n",
            "DEBUG: 'generate_summary' took 5.6126 sec\n",
            "DEBUG: 'generate_summary' took 6.3294 sec\n",
            "DEBUG: 'generate_summary' took 7.0478 sec\n",
            "DEBUG: 'generate_summary' took 7.1001 sec\n",
            "DEBUG: 'generate_summary' took 7.1113 sec\n",
            "DEBUG: 'generate_summary' took 13.8757 sec\n",
            "DEBUG: 'evaluate_textrank_t5' took 76.5662 sec\n",
            "T5 Summarizer\n",
            "Score for each dataset entry\n",
            "[ sample.txt ] - ROUGE 1: 0.308300395256917 2: 0.03187250996015936 L: 0.15810276679841895 BERTScore F1: 0.8202622532844543 P: 0.8207087516784668 R: 0.8198162317276001 \n",
            "[ transcript_sample.txt ] - ROUGE 1: 0.553191489361702 2: 0.38004750593824227 L: 0.43026004728132383 BERTScore F1: 0.8734961152076721 P: 0.8534051179885864 R: 0.894555926322937 \n",
            "[ vietnameselang.txt ] - ROUGE 1: 0.6702127659574467 2: 0.6451612903225806 L: 0.6702127659574467 BERTScore F1: 0.9273393750190735 P: 0.9692762494087219 R: 0.8888809084892273 \n",
            "[ presentationnotes.txt ] - ROUGE 1: 0.4883116883116883 2: 0.2610966057441253 L: 0.31688311688311693 BERTScore F1: 0.8572008013725281 P: 0.8503414392471313 R: 0.8641717433929443 \n",
            "[ genderdisparities_notes.txt ] - ROUGE 1: 0.502283105022831 2: 0.35944700460829493 L: 0.4018264840182649 BERTScore F1: 0.9009816646575928 P: 0.9210228323936462 R: 0.8817940354347229 \n",
            "[ lecturenotes_sample.txt ] - ROUGE 1: 0.5339652448657188 2: 0.3740095087163233 L: 0.4075829383886256 BERTScore F1: 0.8400366306304932 P: 0.8483992218971252 R: 0.8318372368812561 \n",
            "Average Result: \n",
            "ROUGE 1: 0.509377448129384\n",
            "ROUGE 2: 0.341939070881621\n",
            "ROUGE L: 0.3974780198878662\n",
            "BERTScore F1: 0.8698861400286356\n",
            "BERTScore P: 0.8771922687689463\n",
            "BERTScore R: 0.863509347041448\n",
            "TextRank Summarizer\n",
            "Score for each dataset entry\n",
            "[ sample.txt ] - ROUGE 1: 0.3297872340425532 2: 0.06451612903225808 L: 0.1702127659574468 BERTScore F1: 0.8440845608711243 P: 0.8619470596313477 R: 0.8269473910331726 \n",
            "[ transcript_sample.txt ] - ROUGE 1: 0.3466666666666667 2: 0.10762331838565022 L: 0.16 BERTScore F1: 0.8374004364013672 P: 0.8472265005111694 R: 0.8277996778488159 \n",
            "[ vietnameselang.txt ] - ROUGE 1: 0.5333333333333333 2: 0.4606741573033708 L: 0.4999999999999999 BERTScore F1: 0.9017072916030884 P: 0.931861162185669 R: 0.8734437823295593 \n",
            "[ presentationnotes.txt ] - ROUGE 1: 0.456 2: 0.27419354838709675 L: 0.36 BERTScore F1: 0.8656823635101318 P: 0.8946436643600464 R: 0.838537335395813 \n",
            "[ genderdisparities_notes.txt ] - ROUGE 1: 0.4423963133640553 2: 0.22325581395348837 L: 0.31336405529953915 BERTScore F1: 0.8900530338287354 P: 0.9043208360671997 R: 0.8762284517288208 \n",
            "[ lecturenotes_sample.txt ] - ROUGE 1: 0.6591865357643758 2: 0.5316455696202532 L: 0.3786816269284713 BERTScore F1: 0.8831366300582886 P: 0.8871381878852844 R: 0.8791710138320923 \n",
            "Average Result: \n",
            "ROUGE 1: 0.4612283471951641\n",
            "ROUGE 2: 0.2769847561136862\n",
            "ROUGE L: 0.31370974136424284\n",
            "BERTScore F1: 0.8703440527121226\n",
            "BERTScore P: 0.8878562351067861\n",
            "BERTScore R: 0.8536879420280457\n",
            "Textrank + T5 Summarizer\n",
            "Score for each dataset entry\n",
            "[ sample.txt ] - ROUGE 1: 0.3280423280423281 2: 0.06417112299465241 L: 0.1693121693121693 BERTScore F1: 0.83931565284729 P: 0.8542251586914062 R: 0.8249176144599915 \n",
            "[ transcript_sample.txt ] - ROUGE 1: 0.34567901234567905 2: 0.0912863070539419 L: 0.17283950617283952 BERTScore F1: 0.8313054442405701 P: 0.84256911277771 R: 0.8203389644622803 \n",
            "[ vietnameselang.txt ] - ROUGE 1: 0.29239766081871343 2: 0.1301775147928994 L: 0.22222222222222224 BERTScore F1: 0.8358296751976013 P: 0.8725391626358032 R: 0.8020843863487244 \n",
            "[ presentationnotes.txt ] - ROUGE 1: 0.4719101123595506 2: 0.24905660377358493 L: 0.3295880149812734 BERTScore F1: 0.8609450459480286 P: 0.8876587152481079 R: 0.8357923030853271 \n",
            "[ genderdisparities_notes.txt ] - ROUGE 1: 0.41964285714285715 2: 0.15315315315315317 L: 0.24107142857142858 BERTScore F1: 0.8753301501274109 P: 0.884406566619873 R: 0.8664380311965942 \n",
            "[ lecturenotes_sample.txt ] - ROUGE 1: 0.28822495606326887 2: 0.1164021164021164 L: 0.14059753954305798 BERTScore F1: 0.8038686513900757 P: 0.8228330612182617 R: 0.7857587337493896 \n",
            "Average Result: \n",
            "ROUGE 1: 0.35764948779539957\n",
            "ROUGE 2: 0.13404113636172468\n",
            "ROUGE L: 0.2126051468004985\n",
            "BERTScore F1: 0.8410991032918295\n",
            "BERTScore P: 0.860705296198527\n",
            "BERTScore R: 0.8225550055503845\n"
          ]
        }
      ],
      "source": [
        "evaluate_t5()\n",
        "evaluate_textrank()\n",
        "evaluate_textrank_t5()\n",
        "\n",
        "print(\"T5 Summarizer\")\n",
        "print_eval_scores(t5_scores)\n",
        "print(\"TextRank Summarizer\")\n",
        "print_eval_scores(textrank_scores)\n",
        "print(\"Textrank + T5 Summarizer\")\n",
        "print_eval_scores(textrank_t5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbsjXeaGj0Nz"
      },
      "source": [
        "# Interactive Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofhd74l-j3i9"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "st5UJK8EtCVc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "52ac17f8-5fe8-4e0f-faa5-87db50ae1b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://bb4b2a5273da71ca62.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bb4b2a5273da71ca62.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: 'generate_summary' took 199.9725 sec\n",
            "DEBUG: 'generate_summary' took 58.2150 sec\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://bb4b2a5273da71ca62.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def summarize(text, method, control_type, fixed_count, percentage):\n",
        "    if not text.strip():\n",
        "        return \"Please enter input text.\"\n",
        "\n",
        "    total_sentences = len(sent_tokenize(text))\n",
        "    if total_sentences == 0:\n",
        "        return \"No sentences found in input.\"\n",
        "\n",
        "    # Determine number of sentences to use\n",
        "    if control_type == \"Fixed Count\":\n",
        "        num_sentences = fixed_count\n",
        "    elif control_type == \"Percentage\":\n",
        "        num_sentences = max(1, int((percentage / 100) * total_sentences))\n",
        "    else:\n",
        "        return \"Invalid selection for control type.\"\n",
        "\n",
        "    if method == \"TextRank\":\n",
        "        return pytextrank_summarize(text, top_n=num_sentences)\n",
        "    elif method == \"T5\":\n",
        "        return generate_summary(text)\n",
        "    elif method == \"Combination\":\n",
        "        textrank_output = pytextrank_summarize(text, top_n=num_sentences)\n",
        "        return generate_summary(textrank_output)\n",
        "    else:\n",
        "        return \"Invalid method\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 📝 Lecture Note Summarizer with T5 & TextRank\")\n",
        "\n",
        "    input_text = gr.Textbox(lines=10, label=\"Input Text\")\n",
        "\n",
        "    method = gr.Dropdown(choices=[\"TextRank\", \"T5\", \"Combination\"], label=\"Summarization Method\", value=\"T5\")\n",
        "\n",
        "    control_type = gr.Radio(choices=[\"Fixed Count\", \"Percentage\"], label=\"Sentence Selection Method\", value=\"Fixed Count\")\n",
        "\n",
        "    fixed_count_slider = gr.Slider(1, 10, step=1, value=3, label=\"Number of Sentences\")\n",
        "    percentage_slider = gr.Slider(10, 100, step=5, value=30, label=\"Summary Percentage (%)\")\n",
        "\n",
        "    output = gr.Textbox(lines=10, label=\"Summary Output\")\n",
        "\n",
        "    summarize_button = gr.Button(\"Summarize\")\n",
        "\n",
        "    # Logic to show/hide sliders based on choice\n",
        "    control_type.change(\n",
        "        lambda selection: (gr.update(visible=selection==\"Fixed Count\"), gr.update(visible=selection==\"Percentage\")),\n",
        "        inputs=control_type,\n",
        "        outputs=[fixed_count_slider, percentage_slider]\n",
        "    )\n",
        "\n",
        "    summarize_button.click(\n",
        "        fn=summarize,\n",
        "        inputs=[input_text, method, control_type, fixed_count_slider, percentage_slider],\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}